
# -*- coding: utf-8 -*-
"""PredictX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_G-zRprRVS_qb9kIQYKolCT0aQUJnJn
"""

!pip install -q sentence-transformers scikit-learn faiss-cpu

import json
from typing import List, Dict


def load_transcripts(path: str) -> List[Dict]:
    """
    Loads raw conversational transcripts from JSON
    and normalizes structure.
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    transcripts = []

    for item in data["transcripts"]:
        transcripts.append({
            "transcript_id": item["transcript_id"],
            "domain": item["domain"],
            "intent": item["intent"],
            "reason": item["reason_for_call"],
            "conversation": item["conversation"]
        })

    return transcripts


transcripts = load_transcripts("/Conversational_Transcript_Dataset.json")
print(f"Loaded {len(transcripts)} conversations")

def flatten_turns(transcripts):
    """
    Converts conversations into individual turn-level records.
    """
    turns = []

    for transcript in transcripts:
        for idx, turn in enumerate(transcript["conversation"]):
            turns.append({
                "transcript_id": transcript["transcript_id"],
                "turn_id": idx,
                "speaker": turn["speaker"],
                "text": turn["text"],
                "domain": transcript["domain"],
                "intent": transcript["intent"]
            })

    return turns


turns = flatten_turns(transcripts)
print(f"Total dialogue turns: {len(turns)}")
print(turns[0])

from sentence_transformers import SentenceTransformer
import torch

# Decide device automatically
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load a lightweight but very strong model
model = SentenceTransformer(
    "all-MiniLM-L6-v2",
    device=device
)

print("Model loaded on:", device)

# Extract raw text from turns
turn_texts = [turn["text"] for turn in turns]

print("Example turn text:")
print(turn_texts[0])

# Generate embeddings for each dialogue turn
turn_embeddings = model.encode(
    turn_texts,
    batch_size=32,
    show_progress_bar=True,
    convert_to_numpy=True
)

print("Embedding shape:", turn_embeddings.shape)

import numpy as np
from collections import defaultdict

# ---- Build conversation-level embeddings (Stage-1 index) ----
conversation_embeddings = defaultdict(list)

for turn, emb in zip(turns, turn_embeddings):
    conversation_embeddings[turn["transcript_id"]].append(emb)

conversation_ids = []
conversation_vectors = []

for tid, emb_list in conversation_embeddings.items():
    conversation_ids.append(tid)
    conversation_vectors.append(np.mean(emb_list, axis=0))

conversation_vectors = np.vstack(conversation_vectors)

print("Conversation embeddings shape:", conversation_vectors.shape)

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SessionState:
    """
    Explicit, deterministic memory for multi-turn reasoning.
    """
    def __init__(self):
        self.active_query = None
        self.causal_factors = None
        self.evidence = None
        self.outcome_type = None

    def update(self, query, causal_factors, evidence, outcome_type=None):
        self.active_query = query
        self.causal_factors = causal_factors
        self.evidence = evidence
        self.outcome_type = outcome_type

session_state = SessionState()

def handle_followup(query, session_state):
    """
    Handles follow-up analytical queries using stored context.
    """
    if session_state.causal_factors is None:
        return "No prior context found. Please ask an initial query first."

    q = query.lower()

    if "which factor" in q or "most important" in q:
        return {
            "answer": session_state.causal_factors[0],
            "reason": "This factor appeared most frequently across evidence turns."
        }

    if "show evidence" in q or "evidence" in q:
        return {
            "evidence": session_state.evidence
        }

    if "why" in q:
        return {
            "summary": session_state.causal_factors,
            "evidence_count": len(session_state.evidence)
        }

    return "Follow-up query understood, but no specific handler implemented."

def tag_turn(turn_text: str) -> dict:
    """
    Deterministic, rule-based conversational factor tagger.
    Acts as a safe placeholder for an LLM-based annotator.
    """
    text = turn_text.lower()

    return {
        "customer_frustration": (
            "high" if any(w in text for w in ["frustrated", "unacceptable", "wasted", "angry"])
            else "none"
        ),
        "repetition": (
            "yes" if any(w in text for w in ["again", "already", "multiple times"])
            else "no"
        ),
        "escalation_signal": (
            "strong" if any(w in text for w in ["legal", "complaint"])
            else "weak" if any(w in text for w in ["manager", "supervisor"])
            else "none"
        ),
        "agent_action": (
            "apology" if "sorry" in text
            else "explanation" if "because" in text
            else "resolution" if "resolved" in text
            else "none"
        ),
        "policy_reference": (
            "yes" if any(w in text for w in ["policy", "system", "not allowed", "cannot"])
            else "no"
        )
    }

from collections import Counter

def aggregate_factors(tagged_turns):
    """
    Aggregates causal tags across evidence turns.
    Returns frequency counters for each factor.
    """
    counters = {
        "customer_frustration": Counter(),
        "repetition": Counter(),
        "escalation_signal": Counter(),
        "agent_action": Counter(),
        "policy_reference": Counter()
    }

    for t in tagged_turns:
        tags = t["causal_tags"]
        for key in counters:
            counters[key][tags[key]] += 1

    return counters

def extract_dominant_factors(counters, threshold=0.4):
    """
    Converts aggregated counters into dominant causal factors.
    """
    dominant = []

    total = sum(counters["customer_frustration"].values())
    if total == 0:
        return dominant

    if counters["customer_frustration"]["high"] / total >= threshold:
        dominant.append("High customer frustration")

    if counters["repetition"]["yes"] / total >= threshold:
        dominant.append("Repeated unresolved issues")

    if (
        counters["escalation_signal"]["weak"] +
        counters["escalation_signal"]["strong"]
    ) / total >= threshold:
        dominant.append("Explicit escalation signals")

    if counters["agent_action"]["resolution"] / total < threshold:
        dominant.append("Delayed or insufficient resolution")

    return dominant

def infer_outcome_type(query: str) -> str:
    q = query.lower()
    if "escalat" in q:
        return "Escalation"
    if "churn" in q or "cancel" in q:
        return "Churn"
    if "dissatisf" in q or "unhappy" in q:
        return "Dissatisfaction"
    return "Outcome"

def answer_query(query, session_state=None, top_k_conversations=5, top_k_turns=8):
    # ---- Stage 1: Conversation retrieval ----
    query_emb = model.encode(query, convert_to_numpy=True)

    sims = cosine_similarity(
        query_emb.reshape(1, -1),
        conversation_vectors
    )[0]

    top_conv_idx = np.argsort(sims)[::-1][:top_k_conversations]
    selected_convs = {conversation_ids[i] for i in top_conv_idx}

    # ---- Stage 2: Turn retrieval ----
    candidate_turns = []
    candidate_embeddings = []

    for turn, emb in zip(turns, turn_embeddings):
        if turn["transcript_id"] in selected_convs:
            candidate_turns.append(turn)
            candidate_embeddings.append(emb)

    # =====  guard against empty retrieval =====
    if len(candidate_embeddings) == 0:
        if session_state is not None:
            session_state.update(
                query=query,
                causal_factors=[],
                evidence=[],
                outcome_type=infer_outcome_type(query)
            )
        return {
            "query": query,
            "causal_factors": [],
            "evidence": []
        }
    # ===================================================

    candidate_embeddings = np.vstack(candidate_embeddings)

    turn_sims = cosine_similarity(
        query_emb.reshape(1, -1),
        candidate_embeddings
    )[0]

    top_turn_idx = np.argsort(turn_sims)[::-1][:top_k_turns]
    evidence_turns = [candidate_turns[i] for i in top_turn_idx]

    # ---- Factor tagging ----
    tagged_evidence = []
    for t in evidence_turns:
        tagged_evidence.append({
            "transcript_id": t["transcript_id"],
            "turn_id": t["turn_id"],
            "speaker": t["speaker"],
            "text": t["text"],
            "causal_tags": tag_turn(t["text"])
        })

    # ---- Causal aggregation ----
    counters = aggregate_factors(tagged_evidence)
    dominant_factors = extract_dominant_factors(counters)

    explanation = {
        "query": query,
        "causal_factors": dominant_factors,
        "evidence": tagged_evidence
    }

    # ---- Task-2 memory update ----
    if session_state is not None:
        session_state.update(
            query=query,
            causal_factors=dominant_factors,
            evidence=tagged_evidence,
            outcome_type=infer_outcome_type(query)
        )

    return explanation

import pandas as pd

def run_batch_queries(input_csv_path, output_csv_path):
    df = pd.read_csv(input_csv_path)
    outputs = []

    for _, row in df.iterrows():
        session_state = SessionState()
        result = answer_query(row["Query"], session_state)

        system_output = {
            "outcome": session_state.outcome_type,
            "causal_factors": result["causal_factors"],
            "evidence": [
                {
                    "transcript_id": ev["transcript_id"],
                    "turn_id": ev["turn_id"],
                    "speaker": ev["speaker"],
                    "text": ev["text"]
                }
                for ev in result["evidence"]
            ]
        }
        remarks = (
    "No evidence found"
    if len(result["evidence"]) == 0
    else f"Evidence grounded in {len(result['evidence'])} dialogue turns"
)
        outputs.append({
            "Query_Id": row["Query_Id"],
            "Query": row["Query"],
            "Query_Category": row.get("Query_Category", ""),
            "System_Output": json.dumps(system_output, ensure_ascii=False),
            "Remarks": remarks
        })

    pd.DataFrame(outputs).to_csv(output_csv_path, index=False)
    print("Saved:", output_csv_path)

run_batch_queries(
    input_csv_path="queries.csv",
    output_csv_path="submission_output.csv"
)
